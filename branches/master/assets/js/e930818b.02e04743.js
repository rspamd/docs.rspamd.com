"use strict";(self.webpackChunkrspamd_docs=self.webpackChunkrspamd_docs||[]).push([[2107],{28453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>l});var i=s(96540);const t={},o=i.createContext(t);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(o.Provider,{value:n},e.children)}},50282:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"modules/gpt","title":"GPT Plugin","description":"The Rspamd GPT Plugin, introduced in Rspamd 3.9, integrates OpenAI\'s GPT API to enhance spam filtering capabilities using advanced natural language processing techniques. Here are the basic ideas behind this plugin:","source":"@site/docs/modules/gpt.md","sourceDirName":"modules","slug":"/modules/gpt","permalink":"/docs.rspamd.com/branches/master/modules/gpt","draft":false,"unlisted":false,"editUrl":"https://github.com/rspamd/docs.rspamd.com/edit/master/docs/modules/gpt.md","tags":[],"version":"current","frontMatter":{"title":"GPT Plugin"},"sidebar":"docs","previous":{"title":"Fuzzy check module","permalink":"/docs.rspamd.com/branches/master/modules/fuzzy_check"},"next":{"title":"Greylisting module","permalink":"/docs.rspamd.com/branches/master/modules/greylisting"}}');var t=s(74848),o=s(28453);const r={title:"GPT Plugin"},l="Rspamd GPT Plugin",a={},d=[{value:"Configuration Options",id:"configuration-options",level:2},{value:"Description of Configuration Options",id:"description-of-configuration-options",level:3},{value:"Example Configuration",id:"example-configuration",level:2},{value:"Additional Notes",id:"additional-notes",level:2},{value:"JSON vs. Plain-Text Responses",id:"json-vs-plain-text-responses",level:3},{value:"<code>reason_header</code>",id:"reason_header",level:3},{value:"Multiple-Model Ensemble",id:"multiple-model-ensemble",level:3},{value:"Caching Policies",id:"caching-policies",level:3},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"rspamd-gpt-plugin",children:"Rspamd GPT Plugin"})}),"\n",(0,t.jsx)(n.p,{children:"The Rspamd GPT Plugin, introduced in Rspamd 3.9, integrates OpenAI's GPT API to enhance spam filtering capabilities using advanced natural language processing techniques. Here are the basic ideas behind this plugin:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The selected displayed text part is extracted and submitted to the GPT API for spam probability assessment"}),"\n",(0,t.jsx)(n.li,{children:"Additional message details such as Subject, displayed From, and URLs are also included in the assessment"}),"\n",(0,t.jsx)(n.li,{children:"Then, we ask GPT to provide results in JSON format since human-readable GPT output cannot be parsed (in general)"}),"\n",(0,t.jsxs)(n.li,{children:["Some specific symbols (",(0,t.jsx)(n.code,{children:"BAYES_SPAM"}),", ",(0,t.jsx)(n.code,{children:"FUZZY_DENIED"}),", ",(0,t.jsx)(n.code,{children:"REPLY"}),", etc.) are excluded from the GPT scan"]}),"\n",(0,t.jsx)(n.li,{children:"Obvious spam and ham are also excluded from the GPT evaluation"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The last two points reduce the GPT workload for something that is already known, where GPT cannot add any value in the evaluation. We also use GPT as one of the classifiers, meaning that we do not rely solely on GPT evaluation."}),"\n",(0,t.jsx)(n.h2,{id:"configuration-options",children:"Configuration Options"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"By default, the GPT Plugin is disabled."})," To enable the plugin, add the following command in your Rspamd configuration:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-hcl",children:"gpt {\n  enabled = true; # Ensure this line is present to enable the GPT Plugin\n}\n"})}),"\n",(0,t.jsx)(n.p,{children:"The full list of the plugin configuration options:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-hcl",children:'gpt {\n  # Enable the plugin\n  enabled = true;\n\n  # LLM provider type: openai (remote) or ollama (local)\n  type = "openai";\n  \n  # Your OpenAI API key (not required for ollama)\n  api_key = "xxx";\n  \n  # Model name (string or a list for ensemble requests)\n  model = "gpt-5-mini";\n\n  # Per-model parameters (only for openai model)\n  model_parameters = {\n    "gpt-5-mini" = {\n      max_completion_tokens = 1000,\n    },\n    "gpt-5-nano" = {\n      max_completion_tokens = 1000,\n    },\n    "gpt-4o-mini" = {\n      max_tokens = 1000,\n      temperature = 0.0,\n    }\n  };\n\n  # Maximum number of tokens to request (only for ollama model)\n  max_tokens = 1000;\n  \n  # Temperature for sampling (only for ollama model)\n  temperature = 0.0;\n  \n  # Timeout for requests\n  timeout = 10s;\n  \n  # Prompt for the model (use default if not set)\n  prompt = "xxx";\n  \n  # Custom condition (Lua function)\n  condition = "xxx";\n  \n  # Autolearn if GPT classified\n  autolearn = true;\n\n  # Custom Lua function to convert the model\'s reply. Leave unset to use the\n  # built-in parsers for OpenAI / Ollama replies.\n  reply_conversion = "xxx";\n\n  # Header to add with the reason produced by GPT (set to null to disable)\n  reason_header = "X-GPT-Reason";\n\n  # Skip GPT scan if **any** of these symbols are present (and their absolute\n  # weight is equal or above the specified value)\n  symbols_to_except = {\n    BAYES_SPAM = 0.9;\n    WHITELIST_SPF = -1;\n  };\n\n  # Trigger GPT scan only when **all** of these symbols are present (and their\n  # absolute weight is equal or above the specified value)\n  # symbols_to_trigger = {\n  #   URIBL_BLOCKED = 1.0;\n  # };\n\n  # Check messages that resulted in a `passthrough` action\n  allow_passthrough = false;\n\n  # Check messages that already look like ham (negative score)\n  allow_ham = false;\n\n  # Request / expect JSON reply from GPT\n  json = false;\n\n  # Map of extra virtual symbols that could be set from GPT response categories\n  # extra_symbols = {\n  #   GPT_MARKETING = {\n  #     score = 0.0;\n  #     description = "GPT model detected marketing content";\n  #     category = "marketing";\n  #     group = "GPT";\n  #   };\n  # };\n\n  # Prefix for Redis cache keys\n  cache_prefix = "rsllm";\n\n  # Add `response_format = {type = "json_object"}` to requests (OpenAI only)\n  include_response_format = false;\n\n  # API endpoint\n  url = "https://api.openai.com/v1/chat/completions";\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"description-of-configuration-options",children:"Description of Configuration Options"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"enabled"}),": Enables the GPT plugin."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"type"}),": LLM provider. Accepts ",(0,t.jsx)(n.code,{children:"openai"})," (default) or ",(0,t.jsx)(n.code,{children:"ollama"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"api_key"}),": OpenAI API key. Not required for ",(0,t.jsx)(n.code,{children:"ollama"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"model"}),": Model name (string) or a list of names to query in parallel."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"model_parameters"}),": For each model the required parameters. Required for ",(0,t.jsx)(n.code,{children:"openai"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"max_tokens"}),": Maximum number of tokens returned by the model. Required for ",(0,t.jsx)(n.code,{children:"ollama"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"temperature"}),": Sampling temperature. Required for ",(0,t.jsx)(n.code,{children:"ollama"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"timeout"}),": Network timeout for LLM requests."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"prompt"}),": Custom system prompt. If omitted, a sensible default is used."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"condition"}),": A Lua function that decides whether a message should be sent to GPT."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"autolearn"}),": When ",(0,t.jsx)(n.code,{children:"true"}),", messages classified by GPT are added to Bayes learning (",(0,t.jsx)(n.code,{children:"learn_spam"})," / ",(0,t.jsx)(n.code,{children:"learn_ham"}),")."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"reply_conversion"}),": Custom Lua function to convert the model's reply. Leave unset to use the built-in parsers for OpenAI / Ollama replies."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"reason_header"}),": Name of the header added with GPT explanation. Set to ",(0,t.jsx)(n.code,{children:"null"})," to disable entirely."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"symbols_to_except"}),": Table of symbols that ",(0,t.jsx)(n.em,{children:"exclude"})," a message from GPT processing."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"symbols_to_trigger"}),": Table of symbols that must all be present to ",(0,t.jsx)(n.em,{children:"trigger"})," GPT processing."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"allow_passthrough"}),": When ",(0,t.jsx)(n.code,{children:"true"}),", messages with ",(0,t.jsx)(n.code,{children:"passthrough"})," action are still evaluated by GPT."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"allow_ham"}),": When ",(0,t.jsx)(n.code,{children:"true"}),", messages that already look like ham (negative score) are still evaluated."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"json"}),": When ",(0,t.jsx)(n.code,{children:"true"}),", the plugin forces the LLM to return a JSON object and parses it. This can noticeably ",(0,t.jsx)(n.em,{children:"reduce answer quality"})," and is generally ",(0,t.jsx)(n.strong,{children:"not recommended"})," unless you have a very deterministic prompt that the model might otherwise ignore."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra_symbols"}),": Map that allows GPT to set additional virtual symbols based on returned categories."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"cache_prefix"}),": Prefix used for Redis cache keys."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"include_response_format"}),": Adds OpenAI ",(0,t.jsx)(n.code,{children:"response_format = json_object"})," hint. Useful only together with ",(0,t.jsx)(n.code,{children:"json = true"}),", otherwise unnecessary."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"url"}),": API endpoint for the selected provider."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"example-configuration",children:"Example Configuration"}),"\n",(0,t.jsx)(n.p,{children:"Here is a minimal example configuration:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-hcl",children:'gpt {\n  enabled = true;\n  type = "openai";\n  api_key = "your_api_key_here";\n  model = "gpt-5-mini";\n  model_parameters = {\n    "gpt-5-mini" = {\n      max_completion_tokens = 1000,\n    }\n  }\n  timeout = 10s;\n  reason_header = "X-GPT-Reason";\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"additional-notes",children:"Additional Notes"}),"\n",(0,t.jsx)(n.h3,{id:"json-vs-plain-text-responses",children:"JSON vs. Plain-Text Responses"}),"\n",(0,t.jsxs)(n.p,{children:["The plugin works best when the model can answer in plain text because it gives the language model more freedom and generally yields better reasoning. Enabling ",(0,t.jsx)(n.code,{children:"json = true"})," (and optionally ",(0,t.jsx)(n.code,{children:"include_response_format"}),") constrains the model to produce a strict JSON object. This often degrades the probability estimate or makes the reply longer than required, so turn it on only if your prompts really need JSON."]}),"\n",(0,t.jsx)(n.h3,{id:"reason_header",children:(0,t.jsx)(n.code,{children:"reason_header"})}),"\n",(0,t.jsxs)(n.p,{children:["If ",(0,t.jsx)(n.code,{children:"reason_header"})," is set to a non-empty string, the plugin injects a mail header with the explanation sentence produced by GPT (",(0,t.jsx)(n.code,{children:"X-GPT-Reason"})," in the examples). This is convenient for debugging or for downstream systems, but remember that the text is untrusted user input and might reveal information to message recipients. Set the option to ",(0,t.jsx)(n.code,{children:"null"})," to disable the header entirely."]}),"\n",(0,t.jsx)(n.h3,{id:"multiple-model-ensemble",children:"Multiple-Model Ensemble"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"model"})," option can be a list:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-hcl",children:'model = ["gpt-4o-mini", "gpt-5-nano"];\n'})}),"\n",(0,t.jsxs)(n.p,{children:["In this case Rspamd queries all listed models in parallel and applies a ",(0,t.jsx)(n.em,{children:"consensus"})," algorithm:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Each model returns a spam probability."}),"\n",(0,t.jsx)(n.li,{children:"If the majority classifies the message as spam with probability > 0.75, the highest spam probability is used."}),"\n",(0,t.jsx)(n.li,{children:"If the majority classifies it as ham with probability < 0.25, the lowest ham probability is used."}),"\n",(0,t.jsx)(n.li,{children:"Otherwise no GPT symbol is added (no consensus)."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This improves robustness and lets you mix cheap fast models with a slower high-quality one."}),"\n",(0,t.jsx)(n.h3,{id:"caching-policies",children:"Caching Policies"}),"\n",(0,t.jsx)(n.p,{children:"To avoid repeated LLM calls, responses are cached in Redis (if configured). Key facts:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Key: ",(0,t.jsx)(n.code,{children:"<cache_prefix>_<env>_<digest>"})," where ",(0,t.jsx)(n.code,{children:"env"})," depends on ",(0,t.jsx)(n.code,{children:"prompt"}),", ",(0,t.jsx)(n.code,{children:"model"}),", and ",(0,t.jsx)(n.code,{children:"url"}),", and ",(0,t.jsx)(n.code,{children:"digest"})," is the SHA256 of the examined text part (truncated)."]}),"\n",(0,t.jsxs)(n.li,{children:["Default TTL is one hour (",(0,t.jsx)(n.code,{children:"cache_ttl"})," option accepts seconds)."]}),"\n",(0,t.jsxs)(n.li,{children:["Workers coordinate using ",(0,t.jsx)(n.em,{children:"pending"})," markers so that only one request per message is sent even in large clusters."]}),"\n",(0,t.jsx)(n.li,{children:"Changing the prompt, model list, or endpoint automatically invalidates the cache because they are part of the key."}),"\n",(0,t.jsxs)(n.li,{children:["You can tune ",(0,t.jsx)(n.code,{children:"cache_prefix"}),", ",(0,t.jsx)(n.code,{children:"cache_ttl"}),", and other cache options directly inside the ",(0,t.jsx)(n.code,{children:"gpt {}"})," block."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"The Rspamd GPT Plugin integrates OpenAI's GPT models into Rspamd, enhancing its spam filtering capabilities with advanced text processing techniques. By configuring the options above, users can customize the plugin to meet specific requirements, thereby enhancing the efficiency and accuracy of spam filtering within Rspamd."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);